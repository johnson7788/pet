# Pattern-Exploiting Training (PET)
è¯¥repositoryåŒ…å«ä»£ç for [Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference](https://arxiv.org/abs/2001.07676) and [It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://arxiv.org/abs/2009.07118)
è®ºæ–‡ä»‹ç»äº†pattern-exploiting trainingï¼ˆPETï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŠç›‘ç£è®­ç»ƒç¨‹åºï¼Œ
å°†è¾“å…¥ç¤ºä¾‹é‡æ–°ç¼–å†™ä¸ºå¡«ç©ºæ ·å¼çŸ­è¯­ã€‚åœ¨ä½èµ„æºç¯å¢ƒä¸­ï¼Œå°½ç®¡å‚æ•°æ¯”GPT-3å°‘99.9ï¼…ï¼Œä½†PETå’ŒiPETæ˜æ˜¾ä¼˜äºå¸¸è§„ç›‘ç£è®­ç»ƒï¼Œå„ç§åŠç›‘ç£åŸºçº¿ç”šè‡³GPT-3ã€‚ 
PETçš„è¿­ä»£å˜ä½“ï¼ˆiPETï¼‰å¯ä»¥è®­ç»ƒå¤šä»£æ¨¡å‹ï¼Œç”šè‡³å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚

<table>
    <tr>
        <th>#Examples</th>
        <th>Training Mode</th>
        <th>Yelp (Full)</th>
        <th>AG's News</th>
        <th>Yahoo Questions</th>
        <th>MNLI</th>
    </tr>
    <tr>
        <td rowspan="2" align="center"><b>0</b></td>
        <td>unsupervised</td>
        <td align="right">33.8</td>
        <td align="right">69.5</td>
        <td align="right">44.0</td>
        <td align="right">39.1</td>
    </tr>
    <tr>
        <td>iPET</td>
        <td align="right"><b>56.7</b></td>
        <td align="right"><b>87.5</b></td>
        <td align="right"><b>70.7</b></td>
        <td align="right"><b>53.6</b></td>
    </tr>
    <tr>
        <td rowspan="3" align="center"><b>100</b></td>
        <td>supervised</td>
        <td align="right">53.0</td>
        <td align="right">86.0</td>
        <td align="right">62.9</td>
        <td align="right">47.9</td>
    </tr>
    <tr>
        <td>PET</td>
        <td align="right">61.9</td>
        <td align="right">88.3</td>
        <td align="right">69.2</td>
        <td align="right">74.7</td>
    </tr>
    <tr>
        <td>iPET</td>
        <td align="right"><b>62.9</b></td>
        <td align="right"><b>89.6</b></td>
        <td align="right"><b>71.2</b></td>
        <td align="right"><b>78.4</b></td>
    </tr>
</table>
    
<sup>*Note*: To exactly reproduce the above results, make sure to use v1.1.0 (`--branch v1.1.0`).</sup>

## ğŸ“‘ Contents

**[ğŸ”§ Setup](#-setup)**

**[ğŸ’¬ CLI Usage](#-cli-usage)**

**[ğŸ’» API Usage](#-api-usage)**

**[ğŸ¶ Train your own PET](#-train-your-own-pet)**

**[ğŸ“• Citation](#-citation)**

## ğŸ”§ Setup

æœ‰å…³PETçš„æ‰€æœ‰requirementséƒ½å¯ä»¥åœ¨â€œrequirements.txtâ€ä¸­æ‰¾åˆ°ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`pip install -r requirements.txt`å®‰è£…æ‰€æœ‰å¿…éœ€çš„è½¯ä»¶åŒ…ã€‚

## ğŸ’¬ CLI Usage

è¯¥å­˜å‚¨åº“ä¸­çš„å‘½ä»¤è¡Œç•Œé¢cli.pyå½“å‰æ”¯æŒä¸‰ç§ä¸åŒçš„è®­ç»ƒpatternï¼ˆPETï¼ŒiPETï¼Œæœ‰ç›‘ç£çš„è®­ç»ƒï¼‰ï¼Œä¸¤ç§å…¶ä»–è¯„ä¼°æ–¹æ³•ï¼ˆæ— ç›‘ç£å’Œprimingï¼‰ä»¥åŠ13ç§ä¸åŒçš„ä»»åŠ¡ã€‚
æœ‰å…³Yelp Reviewsï¼ŒAG's Newsï¼ŒYahoo Questionsï¼ŒMNLIå’ŒX-Stanceï¼Œè¯·å‚é˜…https://arxiv.org/abs/2001.07676 ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
æœ‰å…³8ä¸ªSuperGLUEä»»åŠ¡ï¼Œè¯·å‚è§https://arxiv.org/abs/2009.07118

### PET Training and Evaluation
è®­ç»ƒå’Œè¯„ä¼°PETæ¨¡å‹, éœ€è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

    python3 cli.py \
	--method pet \
	--pattern_ids $PATTERN_IDS \
	--data_dir $DATA_DIR \
	--model_type $MODEL_TYPE \
	--model_name_or_path $MODEL_NAME_OR_PATH \
	--task_name $TASK \
	--output_dir $OUTPUT_DIR \
	--do_train \
	--do_eval
    
 å…¶ä¸­ 
 - `$PATTERN_IDS` æŒ‡å®šè¦ä½¿ç”¨çš„PVPã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¦ä½¿ç”¨ *all* patternï¼Œåˆ™ä¸ºAG's News and Yahoo QuestionsæŒ‡å®š`PATTERN_IDS 0 1 2 3 4`ï¼Œä¸º Yelp Reviews and MNLIæŒ‡å®š`PATTERN_IDS 0 1 2 3`ã€‚
 - `$DATA_DIR`  æ˜¯åŒ…å«è®­ç»ƒå’Œæµ‹è¯•æ–‡ä»¶çš„ç›®å½•ï¼ˆæ£€æŸ¥tasks.pyä»¥æŸ¥çœ‹å¦‚ä½•ä¸ºæ¯ä¸ªä»»åŠ¡å‘½åå’Œæ ¼å¼åŒ–è¿™äº›æ–‡ä»¶ï¼‰ã€‚
 - `$MODEL_TYPE`  æ˜¯æ‰€ä½¿ç”¨æ¨¡å‹çš„åç§°ï¼Œä¾‹å¦‚`albert`ï¼Œ`bert`æˆ–`roberta`ã€‚
 - `$MODEL_NAME` æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„åç§°ï¼ˆä¾‹å¦‚ï¼Œ `roberta-large` or `albert-xxlarge-v2`ï¼‰æˆ–é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
 - `$TASK_NAME`   æ˜¯è¦è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„ä»»åŠ¡çš„åç§°ã€‚
 - `$OUTPUT_DIR`  æ˜¯ä¿å­˜ç»è¿‡è®­ç»ƒçš„æ¨¡å‹å’Œè¯„ä¼°ç»“æœçš„ç›®å½•çš„åç§°ã€‚
 
æ‚¨è¿˜å¯ä»¥ä¸ºå¯¹åº”äºå„ä¸ªPVPçš„PETæ¨¡å‹é›†åˆï¼ˆå‰ç¼€`--pet_`ï¼‰å’Œæœ€ç»ˆåºåˆ—åˆ†ç±»æ¨¡å‹ï¼ˆå‰ç¼€`--sc_`ï¼‰æŒ‡å®šå„ç§è®­ç»ƒå‚æ•°ã€‚
ä¾‹å¦‚ï¼Œç”¨äºæˆ‘ä»¬çš„SuperGLUEè¯„ä¼°çš„é»˜è®¤å‚æ•°ä¸ºï¼š
 
 	--pet_per_gpu_eval_batch_size 8 \
	--pet_per_gpu_train_batch_size 2 \
	--pet_gradient_accumulation_steps 8 \
	--pet_max_steps 250 \
	--pet_max_seq_length 256 \
    --pet_repetitions 3 \
	--sc_per_gpu_train_batch_size 2 \
	--sc_per_gpu_unlabeled_batch_size 2 \
	--sc_gradient_accumulation_steps 8 \
	--sc_max_steps 5000 \
	--sc_max_seq_length 256 \
    --sc_repetitions 1
    
å¯¹äºæ¯ä¸ªpattern `$P`å’Œrepetition `$I`ï¼Œè¿è¡Œä¸Šé¢çš„å‘½ä»¤å°†åˆ›å»ºä¸€ä¸ªç›®å½•`$OUTPUT_DIR/p$P-i$I`ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š
  - `pytorch_model.bin`: ç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼Œå¯èƒ½è¿˜ä¼šåŒ…å«ä¸€äº›ç‰¹å®šäºæ¨¡å‹çš„æ–‡ä»¶(e.g, `spiece.model`, `special_tokens_map.json`)
  - `wrapper_config.json`: æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹çš„é…ç½®
  - `train_config.json`: ç”¨äºè®­ç»ƒçš„é…ç½®
  - `eval_config.json`: ç”¨äºè¯„ä¼°çš„é…ç½®
  - `logits.txt`: æ¨¡å‹å¯¹æ— æ ‡ç­¾æ•°æ®çš„é¢„æµ‹
  - `eval_logits.txt`: æ¨¡å‹å¯¹è¯„ä¼°æ•°æ®çš„é¢„æµ‹
  - `results.json`: ï¼šä¸€ä¸ªåŒ…å«ç»“æœçš„jsonæ–‡ä»¶ï¼Œä¾‹å¦‚æ¨¡å‹çš„æœ€ç»ˆç²¾åº¦
  - `predictions.jsonl`: SuperGlueæ ¼å¼çš„è¯„ä¼°é›†çš„é¢„æµ‹æ–‡ä»¶
  
æ¯æ¬¡epetition `$I` çš„æœ€ç»ˆï¼ˆè’¸é¦ï¼‰æ¨¡å‹å¯ä»¥åœ¨`$OUTPUT_DIR/final/p0-i$I`ä¸­æ‰¾åˆ°ï¼Œè¯¥æ¨¡å‹åŒ…å«ä¸ä¸Šè¿°ç›¸åŒçš„æ–‡ä»¶ã€‚

ğŸš¨ å¦‚æœæ‚¨çš„GPUåœ¨è®­ç»ƒæœŸé—´å†…å­˜ä¸è¶³ï¼Œåˆ™å¯ä»¥å°è¯•åŒæ—¶å‡ `pet_per_gpu_train_batch_size` and the `sc_per_gpu_unlabeled_batch_size`
 , åŒæ—¶å¢åŠ  `pet_gradient_accumulation_steps` and `sc_gradient_accumulation_steps`.


### iPET Training and Evaluation

è¦ä¸ºå…¶ä¸­ä¸€ä¸ªä»»åŠ¡è®­ç»ƒå’Œè¯„ä¼°iPETæ¨¡å‹ï¼Œåªéœ€è¿è¡Œä¸ä¸Šè¿°ç›¸åŒçš„å‘½ä»¤ï¼Œ
ç„¶åå°†`--method pet` æ›¿æ¢ä¸º`--method ipet`å³å¯ã€‚æ‚¨å¯ä»¥ä¿®æ”¹å„ç§å…¶ä»–çš„iPETå‚æ•°ã€‚å®ƒä»¬éƒ½ä»¥`--ipet_`ä¸ºå‰ç¼€ã€‚

å¯¹äºgeneration `$G`ï¼Œpattern `$P` and iteration `$I`ï¼Œ
è¿™å°†åˆ›å»ºç›®å½•`$OUTPUT_DIR/g$G/p$P-i$I` ï¼Œå…¶ç»“æ„ä¸å¸¸è§„PETç›¸åŒã€‚æœ€ç»ˆï¼ˆæå–çš„ï¼‰æ¨¡å‹å¯ä»¥å†æ¬¡åœ¨`$OUTPUT_DIR/final/p0-i$I`.ä¸­æ‰¾åˆ°ã€‚

ğŸš¨å¦‚æœå°†iPETä¸zeroä¸ªè®­ç»ƒç¤ºä¾‹ä¸€èµ·ä½¿ç”¨ï¼Œåˆ™éœ€è¦æŒ‡å®šåœ¨ç¬¬ä¸€generationä¸­åº”ä¸ºæ¯ä¸ªæ ‡ç­¾é€‰æ‹©å¤šå°‘ä¸ªç¤ºä¾‹ï¼Œ
å¹¶ä¸”éœ€è¦å°†å‡å°‘ç­–ç•¥æ›´æ”¹ä¸ºï¼š`--ipet_n_most_likely 100 --reduction mean`.

### Supervised Training and Evaluation

è¦ä»¥ç›‘ç£çš„æ–¹å¼è®­ç»ƒå’Œè¯„ä¼°å¸¸è§„åºåˆ—åˆ†ç±»å™¨ï¼Œåªéœ€è¿è¡Œä¸ä¸Šè¿°ç›¸åŒçš„å‘½ä»¤ï¼Œ
ä½†æ˜¯å°†`--method pet` æ›¿æ¢ä¸º`--method sequence_classifier`å³å¯ã€‚æ‚¨å¯ä»¥ä¿®æ”¹åºåˆ—åˆ†ç±»å™¨çš„å„ç§å…¶ä»–å‚æ•°ã€‚å®ƒä»¬éƒ½ä»¥`--sc_`ä¸ºå‰ç¼€ã€‚

### Unsupervised Evaluation

è¦ä½¿ç”¨é»˜è®¤çš„PET patternå’Œverbalizers è¯„ä¼°ç»è¿‡é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œä½†æ— éœ€è¿›è¡Œå¾®è°ƒï¼Œ
è¯·åˆ é™¤å‚æ•°`--do_train`å¹¶æ·»åŠ `--no_distillation`ï¼Œä»¥ä¾¿ä¸æ‰§è¡Œæœ€ç»ˆçš„è’¸é¦ã€‚

### Priming

å¦‚æœè¦ä½¿ç”¨primingï¼Œè¯·åˆ é™¤å‚æ•° `--do_train` å¹¶æ·»åŠ å‚æ•° `--priming --no_distillation`ï¼Œä»¥ä¾¿æ‰€æœ‰è®­ç»ƒæ ·æœ¬å‡ç”¨äºprimingï¼Œå¹¶ä¸”ä¸æ‰§è¡Œæœ€ç»ˆè’¸é¦ã€‚

ğŸš¨è¯·è®°ä½ï¼Œæ‚¨å¯èƒ½éœ€è¦å°†æœ€å¤§åºåˆ—é•¿åº¦å¢åŠ åˆ°æ›´å¤§çš„å€¼ï¼Œä¾‹å¦‚`--pet_max_seq_length 5000`ã€‚
è¿™ä»…é€‚ç”¨äºæ”¯æŒæ­¤ç±»é•¿åºåˆ—çš„è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚XLNetã€‚ä¸ºäº†ä½¿ç”¨XLNetï¼Œæ‚¨å¯ä»¥æŒ‡å®š`--model_type xlnet --model_name_or_path xlnet-large-cased --wrapper_type plm`ã€‚

## ğŸ’» API Usage

é™¤äº†ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢ä¹‹å¤–ï¼Œæ‚¨è¿˜å¯ä»¥ç›´æ¥ä½¿ç”¨PET APIï¼Œå…¶ä¸­å¤§å¤šæ•°åœ¨`pet.modeling`ä¸­å®šä¹‰ã€‚
é€šè¿‡åŒ…å«`import pet`ï¼Œæ‚¨å¯ä»¥è®¿é—®è¯¸å¦‚`train_pet`, `train_ipet` and `train_classifier`.ä¹‹ç±»çš„æ–¹æ³•ã€‚æŸ¥çœ‹ä»–ä»¬çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## ğŸ¶ Train your own PET

è¦å°†PETç”¨äºè‡ªå®šä¹‰ä»»åŠ¡ï¼Œæ‚¨éœ€è¦å®šä¹‰ä¸¤ä»¶äº‹ï¼š

- a **DataProcessor**, è´Ÿè´£åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°. See `examples/custom_task_processor.py` for an example.
- a **PVP**, è´Ÿè´£å°†patternåº”ç”¨åˆ°è¾“å…¥å¹¶å°†æ ‡ç­¾æ˜ å°„åˆ°è‡ªç„¶è¯­è¨€verbalizations. See `examples/custom_task_pvp.py` for an example.

åœ¨å®ç°äº†DataProcessorå’ŒPVPä¹‹åï¼Œæ‚¨å¯ä»¥å¦‚ä¸Šæ‰€è¿°ä½¿ç”¨å‘½ä»¤è¡Œæ¥è®­ç»ƒPETæ¨¡å‹[described above](#pet-training-and-evaluation)ï¼‰ã€‚
åœ¨ä¸‹é¢ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°æœ‰å…³å¦‚ä½•å®šä¹‰PVPçš„ä¸¤ä¸ªç»„ä»¶*verbalizers* and *patterns*çš„å…¶ä»–ä¿¡æ¯ã€‚

### Verbalizers

Verbalizersç”¨äºå°†ä»»åŠ¡æ ‡ç­¾æ˜ å°„åˆ°è‡ªç„¶è¯­è¨€çš„å•è¯ã€‚
ä¾‹å¦‚ï¼Œåœ¨äºŒè¿›åˆ¶æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ‚¨å¯ä»¥å°†æ­£æ ‡ç­¾ï¼ˆ+1ï¼‰æ˜ å°„åˆ°å•è¯`good`ï¼Œå°†è´Ÿæ ‡ç­¾ï¼ˆ-1ï¼‰æ˜ å°„åˆ°`bad`ã€‚
Verbalizersæ˜¯é€šè¿‡PVPçš„`verbalize()`æ–¹æ³•å®ç°çš„ã€‚å®šä¹‰verbalizerçš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨å­—å…¸ï¼š
```python
VERBALIZER = {"+1": ["good"], "-1": ["bad"]}
    
def verbalize(self, label) -> List[str]:
    return self.VERBALIZER[label]       
```
é‡è¦çš„æ˜¯ï¼Œåœ¨PETçš„å½“å‰ç‰ˆæœ¬ä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œverbalizersä»…é™äºåŸºç¡€LMå•è¯è¡¨ä¸­çš„ **single tokens** ï¼ˆè¦ä½¿ç”¨å¤šä¸ªtokenï¼Œ[see below](#pet-with-multiple-masks))ã€‚
ç»™å®šè¯­è¨€æ¨¡å‹çš„tokenizerï¼Œæ‚¨å¯ä»¥é€šè¿‡éªŒè¯`len(tokenizer.tokenize(word)) == 1`.æ¥è½»æ¾æ£€æŸ¥å•è¯æ˜¯å¦ä¸å•ä¸ªæ ‡ç­¾ç›¸å¯¹åº”ã€‚

æ‚¨è¿˜å¯ä»¥ä¸ºå•ä¸ªæ ‡ç­¾å®šä¹‰å¤šä¸ªverbalizationsã€‚
ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä¸ç¡®å®šå“ªä¸ªè¯æœ€èƒ½ä»£è¡¨äºŒè¿›åˆ¶æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­çš„æ ‡ç­¾ï¼Œåˆ™å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼å®šä¹‰verbalizerï¼š

```python
VERBALIZER = {"+1": ["great", "good", "wonderful", "perfect"], "-1": ["bad", "terrible", "horrible"]}
```

### Patterns

patternç”¨äºä½¿è¯­è¨€æ¨¡å‹ç†è§£ç»™å®šçš„ä»»åŠ¡ã€‚
å®ƒä»¬å¿…é¡»åªåŒ…å«ä¸€ä¸ª`<MASK>` tokenï¼Œè¯¥tokenå°†ä½¿ç”¨verbalizerå¡«å……ã€‚
å¯¹äºåŸºäºè¯„è®ºæ‘˜è¦ï¼ˆ`<A>`ï¼‰å’Œæ­£æ–‡ï¼ˆ`<B>`ï¼‰çš„äºŒè¿›åˆ¶æƒ…æ„Ÿåˆ†ç±»ï¼Œ
åˆé€‚çš„patternå¯ä»¥æ˜¯`<A>. <B>`ã€‚æ€»çš„æ¥è¯´ï¼Œå®ƒæ˜¯<MASK>ã€‚
patternæ˜¯é€šè¿‡PVPçš„`get_parts()`æ–¹æ³•å®ç°çš„ï¼Œè¯¥æ–¹æ³•è¿”å›ä¸€å¯¹æ–‡æœ¬åºåˆ—ï¼ˆæ¯ä¸ªåºåˆ—ç”±å­—ç¬¦ä¸²åˆ—è¡¨è¡¨ç¤ºï¼‰ï¼š

```python
def get_parts(self, example: InputExample):
    return [example.text_a, '.', example.text_b, '.'], ['Overall, it was ', self.mask]
```
å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨ä¸€å¯¹åºåˆ—ï¼Œåˆ™åªéœ€å°†ç¬¬äºŒä¸ªåºåˆ—ç•™ç©ºï¼š

```python
def get_parts(self, example: InputExample):
    return [example.text_a, '.', example.text_b, '. Overall, it was ', self.mask], []
```

å¦‚æœè¦å®šä¹‰å‡ ç§patternï¼Œåªéœ€ä½¿ç”¨PVPçš„pattern_idå±æ€§ï¼š            

```python
def get_parts(self, example: InputExample):
    if self.pattern_id == 1:
        return [example.text_a, '.', example.text_b, '.'], ['Overall, it was ', self.mask]
    elif self.pattern_id == 2:
        return ['It was just ', self.mask, '!', example.text_a, '.', example.text_b, '.'], []
```

ä½¿ç”¨å‘½ä»¤è¡Œè®­ç»ƒæ¨¡å‹æ—¶ï¼Œè¯·æŒ‡å®šè¦ä½¿ç”¨çš„æ‰€æœ‰patternï¼ˆä¾‹å¦‚ï¼Œ`--pattern_ids 1 2`).

é‡è¦çš„æ˜¯ï¼Œå¦‚æœåºåˆ—é•¿äºåŸºç¡€LMçš„æŒ‡å®šæœ€å¤§åºåˆ—é•¿åº¦ï¼Œ
åˆ™PETå¿…é¡»çŸ¥é“è¾“å…¥çš„å“ªäº›éƒ¨åˆ†å¯ä»¥ç¼©çŸ­è€Œå“ªäº›éƒ¨åˆ†ä¸èƒ½ç¼©çŸ­ï¼ˆä¾‹å¦‚ï¼Œmask tokenå¿…é¡»å§‹ç»ˆå­˜åœ¨ï¼‰ã€‚
å› æ­¤ï¼ŒPVPæä¾›äº†`shortenable()` æ–¹æ³•æ¥æŒ‡ç¤ºå¯ä»¥ç¼©çŸ­ä¸€æ®µæ–‡æœ¬ï¼š

```python
def get_parts(self, example: InputExample):
    text_a = self.shortenable(example.text_a)
    text_b = self.shortenable(example.text_b)
    return [text_a, '.', text_b, '. Overall, it was ', self.mask], []
```

### PET with Multiple Masks

é»˜è®¤æƒ…å†µä¸‹ï¼ŒPETå’ŒiPETçš„å½“å‰å®ç°ä»…æ”¯æŒä¸€ç»„å›ºå®šçš„æ ‡ç­¾ï¼Œè¯¥æ ‡ç­¾åœ¨ä¸å•ä¸ªtokenç›¸å¯¹åº”çš„æ‰€æœ‰æ ·æœ¬å’Œverbalizersä¹‹é—´å…±äº«ã€‚
å¦‚æœè¦ä½¿ç”¨å¯¹åº”äºå¤šä¸ªtokençš„verbalizers, å¦‚æ­¤å¤„æ‰€è¿°, http://arxiv.org/abs/2009.07118ï¼Œ
åˆ™éœ€è¦å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰`TaskHelper`å¹¶å°†å…¶æ·»åŠ åˆ°`TASK_HELPERS`å­—å…¸ä¸­åœ¨`pet / tasks.py`ä¸­ã€‚
é¦–å…ˆï¼Œæ‚¨å¯ä»¥åœ¨`pet/task_helpers.py`ä¸­æ£€å‡º`CopaTaskHelper`, `WscTaskHelper` and `RecordTaskHelper` ç±»ã€‚
åœ¨ä¸‹ä¸€ç‰ˆçš„PETä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹å°†å¯ç”¨å¸¦æœ‰å¤šä¸ªmasksçš„verbalizersã€‚

## ğŸ“• Citation

If you make use of the code in this repository, please cite the following papers:

    @article{schick2020exploiting,
      title={Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference},
      author={Timo Schick and Hinrich SchÃ¼tze},
      journal={Computing Research Repository},
      volume={arXiv:2001.07676},
      url={http://arxiv.org/abs/2001.07676},
      year={2020}
    }

    @article{schick2020small,
      title={It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
      author={Timo Schick and Hinrich SchÃ¼tze},
      journal={Computing Research Repository},
      volume={arXiv:2009.07118},
      url={http://arxiv.org/abs/2009.07118},
      year={2020}
    }
